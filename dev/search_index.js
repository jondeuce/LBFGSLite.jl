var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = LBFGSLite","category":"page"},{"location":"#LBFGSLite","page":"Home","title":"LBFGSLite","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LBFGSLite.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [LBFGSLite]","category":"page"},{"location":"#LBFGSLite.LBFGSParams","page":"Home","title":"LBFGSLite.LBFGSParams","text":"struct LBFGSParams{T <: AbstractFloat}\n\nParameter struct for LBFGS.\n\nArguments\n\nmem_size::Int = 8: The number of corrections to approximate the inverse hessian matrix. The L-BFGS routine stores the computation results of previous m iterations to approximate the inverse hessian matrix of the current iteration. This parameter controls the size of the limited memories (corrections). Values less than 3 are not recommended. Large values will result in excessive computing time.\ng_epsilon::T = 1.0e-5: Epsilon for grad convergence test. DO NOT USE IT in nonsmooth cases! Set it to 0.0 and use past-delta-based test for nonsmooth functions. This parameter determines the accuracy with which the solution is to be found. A minimization terminates when ||g(x)||∞ / max(1, ||x||∞) < gepsilon, where ||.||∞ is the infinity norm. This should be greater than 1.0e-6 in practice because L-BFGS does not directly reduce first-order residual. It still needs the function value which can be corrupted by machine_prec when ||g|| is small.\npast::Int = 3: Distance for delta-based convergence test. This parameter determines the distance, in iterations, to compute the rate of decrease of the cost function. If the value of this parameter is zero, the library does not perform the delta-based convergence test.\ndelta::T = 1.0e-6: Delta for convergence test. This parameter determines the minimum rate of decrease of the cost function. The library stops iterations when the following condition is met: |f' - f| / max(1, |f|) < delta, where f' is the cost value of past iterations ago, and f is the cost value of the current iteration.\nmax_iterations::Int = 0: The maximum number of iterations. The optimize function terminates an minimization process with st::LBFGSERR_MAXIMUMITERATION status code when the iteration count exceedes this parameter. Setting this parameter to zero continues a minimization process until a convergence or error.\nmax_linesearch::Int = 64: The maximum number of trials for the line search. This parameter controls the number of function and gradients evaluations per iteration for the line search routine.\nmin_step::T = 1.0e-20: The minimum step of the line search routine. This value need not be modified unless the exponents are too large for the machine being used, or unless the problem is extremely badly scaled (in which case the exponents should be increased).\nmax_step::T = 1.0e+20: The maximum step of the line search. This value need not be modified unless the exponents are too large for the machine being used, or unless the problem is extremely badly scaled (in which case the exponents should be increased).\nf_dec_coeff::T = 1.0e-4: A parameter to control the accuracy of the line search routine. This parameter should be greater than zero and smaller than 1.0.\ns_curv_coeff::T = 0.9: A parameter to control the accuracy of the line search routine. If the function and gradient evaluations are inexpensive with respect to the cost of the iteration (which is sometimes the case when solving very large problems) it may be advantageous to set this parameter to a small value. A typical small value is 0.1. This parameter should be greater than the fdeccoeff parameter and smaller than 1.0.\ncautious_factor::T = 1.0e-6: A parameter to ensure the global convergence for nonconvex functions. The parameter performs the so called cautious update for L-BFGS, especially when the convergence is not sufficient. The parameter must be positive but might as well be less than 1.0e-3 in practice.\n\n\n\n\n\n","category":"type"},{"location":"#LBFGSLite.optimize","page":"Home","title":"LBFGSLite.optimize","text":"optimize(\n    fg!::F\n    x::AbstractVector{T},\n    params::LBFGSParams{T} = LBFGSParams{T}(),\n) = optimize!(fg!, copy(x), LBFGSWorkspace(x, params), params)\n\noptimize!(\n    fg!::F\n    x::AbstractVector{T},\n    work::LBFGSWorkspace{T},\n    params::LBFGSParams{T} = LBFGSParams{T}(),\n) -> x, fx::T, st::LBFGS_STATUS\n\nMinimize a function using L-BFGS.\n\nAssumptions:\n\nf(x) is either C2 or C0 but piecewise C2;\nf(x) is lower bounded;\nf(x) has bounded level sets;\ng(x) is either the gradient or subgradient;\nThe gradient exists at the initial guess x0.\n\nArguments\n\nfg!(x, g)::F: In-place function that computes the objective fx = fg!(x, g) and stores the gradient in g.\nx::AbstractVector{T}: Vector of decision variables. Used for the initial guess.\nwork::LBFGSWorkspace{T}: Struct with pre-allocated arrays.\nparams::LBFGSParams{T} = LBFGSParams{T}(): The parameters for L-BFGS optimization.\n\nReturns\n\nx::AbstractVector{T}: Vector of decision variables.\nfx::T: Final value of cost function.\nst::LBFGS_STATUS: Status code.\n\n\n\n\n\n","category":"function"}]
}
